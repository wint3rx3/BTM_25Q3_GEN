from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json
from tqdm import tqdm
import re

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - í‚¤ì›Œë“œ ì¶”ì¶œìš©
KEYWORD_SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ í•œêµ­ì–´ ë§ì¶¤ë²•Â·ë¬¸ì¥ë¶€í˜¸ êµì • ë‹µë³€ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
    "ì£¼ì–´ì§„ ë‹µë³€ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì–´ë¬¸ ê·œë²” ê´€ë ¨ í‚¤ì›Œë“œë¥¼ 1~3ê°œ ì¶”ì¶œí•˜ì„¸ìš”.\n"
    "í‚¤ì›Œë“œëŠ” ë‹¨ì–´ë‚˜ ì§§ì€ êµ¬ë¬¸ í˜•íƒœë¡œ, ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.\n"
    "ì˜ˆì‹œ: ë„ì–´ì“°ê¸°, ì™¸ë˜ì–´ í‘œê¸°ë²•, ë§ì¶¤ë²•"
)

# tokenizer ë° base model ë¡œë”© (train.pyì™€ ë™ì¼í•œ ë°©ì‹)
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-8B", device_map="auto", torch_dtype=torch.float16, trust_remote_code=True)
model.eval()
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

# predictions.json íŒŒì¼ ë¡œë”©
def load_predictions(file_path):
    """predictions.json íŒŒì¼ì„ ë¡œë”©í•©ë‹ˆë‹¤."""
    try:
        with open(file_path, encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

# answerì—ì„œ í•„ìš”í•œ ë¶€ë¶„ ì¶”ì¶œ
def extract_answer_part(answer):
    """answerì—ì„œ 'ì˜³ë‹¤' ë¶€ë¶„ê³¼ ì´ìœ  ë¶€ë¶„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        # 'ì˜³ë‹¤'ê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶”ì¶œ
        pat = r'(?:^|(?<=[.!?â€¦ã€‚ï¼ï¼Ÿ]))\s*(?:(?:"[^"]*"|\'[^\']*\'|[â€œ][^â€]*[â€]|[â€˜][^â€™]*[â€™]))?[^.!?â€¦ã€‚ï¼ï¼Ÿ]*ì˜³ë‹¤[^.!?â€¦ã€‚ï¼ï¼Ÿ]*[.!?â€¦ã€‚ï¼ï¼Ÿ]'
        m = re.search(pat, answer)
        if m:
            correct_part = m.group(0)
        else:
            correct_part = ""
        
        # 'ì˜³ë‹¤.' ë’¤ì˜ ì´ìœ  ë¶€ë¶„ ì¶”ì¶œ
        m = re.search(r'(?<=ì˜³ë‹¤\.)\s*(.*)$', answer, flags=re.S)
        if m:
            reason = m.group(1)
        else:
            reason = ""
        
        # ì¶”ì¶œëœ ë¶€ë¶„ì„ í•©ì³ì„œ ë°˜í™˜
        extracted_text = (correct_part + " " + reason).strip()
        return extracted_text if extracted_text else answer
    except:
        return answer

# í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ìƒì„±
def make_keyword_prompt(answer_text):
    """ë‹µë³€ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return f"ë‹¤ìŒ í•œêµ­ì–´ ë§ì¶¤ë²•Â·ë¬¸ì¥ë¶€í˜¸ êµì • ë‹µë³€ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 1~3ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:\n\n{answer_text}"

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords_from_answer(answer_text):
    """ë‹¨ì¼ ë‹µë³€ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # answerì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    processed_answer = extract_answer_part(answer_text)
    
    messages = [
        {"role": "system", "content": KEYWORD_SYSTEM_PROMPT},
        {"role": "user", "content": make_keyword_prompt(processed_answer)}
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True,
        enable_thinking=False  # thinking ëª¨ë“œ ë¹„í™œì„±í™”
    )
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=50,  # í‚¤ì›Œë“œëŠ” ì§§ìœ¼ë¯€ë¡œ í† í° ìˆ˜ ì¤„ì„
            do_sample=True,  # non-thinking ëª¨ë“œì—ì„œëŠ” do_sample=True ê¶Œì¥
            pad_token_id=tokenizer.eos_token_id,
            temperature=0.7,  # non-thinking ëª¨ë“œ ê¶Œì¥ê°’
            top_p=0.8,        # non-thinking ëª¨ë“œ ê¶Œì¥ê°’
            top_k=20,         # non-thinking ëª¨ë“œ ê¶Œì¥ê°’
            min_p=0.0         # non-thinking ëª¨ë“œ ê¶Œì¥ê°’
        )

    generated_tokens = output[0][inputs["input_ids"].shape[1]:]
    decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
    
    # í‚¤ì›Œë“œ ì •ì œ (ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ê³µë°± ì œê±°)
    keywords = [kw.strip() for kw in decoded.split(',') if kw.strip()]
    return keywords[:3]  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ

# ì „ì²´ predictionsì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords_batch(predictions_data):
    """ì „ì²´ predictions ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    results = []
    
    for i, item in enumerate(tqdm(predictions_data, desc="í‚¤ì›Œë“œ ì¶”ì¶œ ì§„í–‰")):
        answer_text = item["output"]["answer"]
        keywords = extract_keywords_from_answer(answer_text)
        
        # ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì¶œë ¥
        print(f"\n=== í•­ëª© {i+1} (ID: {item['id']}) ===")
        print(f"ì›ë³¸ ë‹µë³€: {answer_text}")
        print(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords)}")
        print("-" * 50)
        
        result_item = {
            "id": item["id"],
            "question_type": item["input"]["question_type"],
            "keywords": keywords
        }
        results.append(result_item)
    
    return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # predictions.json íŒŒì¼ ê²½ë¡œ (í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°)
    predictions_files = ["predictions.json", "predictions (1).json"]
    
    predictions_data = None
    used_file = None
    
    for file_path in predictions_files:
        predictions_data = load_predictions(file_path)
        if predictions_data:
            used_file = file_path
            break
    
    if not predictions_data:
        print("âŒ predictions.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“„ ì‚¬ìš© íŒŒì¼: {used_file}")
    print(f"ğŸ“Š ì´ ë°ì´í„° ìˆ˜: {len(predictions_data)}")
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰
    print("\nğŸ”„ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘...")
    keyword_results = extract_keywords_batch(predictions_data)
    
    # ê²°ê³¼ ì €ì¥
    output_file = "extracted_keywords.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(keyword_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ! ê²°ê³¼ ì €ì¥: {output_file}")

if __name__ == "__main__":
    main()
