"""
Method 1: ë£° ë² ì´ìŠ¤ ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰
- í˜•íƒœì†Œ ë¶„ì„ê¸°(KoNLPy Okt) ì‚¬ìš©
- ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œí•˜ì—¬ í‚¤ì›Œë“œ ìƒì„±
- í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ì‹œìŠ¤í…œ
- ì„ê³„ê°’ ê¸°ë°˜ í•„í„°ë§ (70ì  ì´ìƒ)
"""

import json
import re
import pandas as pd
import numpy as np
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class KeywordRuleBasedSearch:
    def __init__(self, data_paths=None, threshold=70):
        """
        í‚¤ì›Œë“œ ë£° ë² ì´ìŠ¤ ê²€ìƒ‰ ì´ˆê¸°í™”
        
        Args:
            data_paths (dict): ë°ì´í„° íŒŒì¼ ê²½ë¡œë“¤
            threshold (float): ì ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 70, 0~100 ë²”ìœ„)
        """
        self.okt = Okt()
        self.threshold = threshold
        self.df = None
        self.entire_examples_keyword_no_dash = []
        self.entire_examples_no_dash_with_index = []
        self.example_to_indices = {}
        
        if data_paths:
            self.load_data(data_paths)
    
    def load_data(self, data_paths):
        """
        ê·œì • ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        
        Args:
            data_paths (dict): ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
                - hangeul_rule: í•œê¸€ ë§ì¶¤ë²• ê·œì • íŒŒì¼ ê²½ë¡œ
                - standard_rule: í‘œì¤€ì–´ ê·œì • íŒŒì¼ ê²½ë¡œ  
                - spacing_rule: ë„ì–´ì“°ê¸° ê·œì • íŒŒì¼ ê²½ë¡œ
        """
        all_data = []
        
        # ê° ê·œì • íŒŒì¼ ë¡œë“œ
        for key, path in data_paths.items():
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_data.extend(data)
        
        # DataFrame ìƒì„± ë° ì „ì²˜ë¦¬
        self.df = pd.DataFrame(all_data)
        self.df['keywords'] = self.df['keywords'].apply(
            lambda x: ', '.join(x) if isinstance(x, list) else str(x)
        )
        self.df['conditions'] = self.df['conditions'].apply(
            lambda x: ', '.join(x) if isinstance(x, list) else str(x)
        )
        self.df['index'] = self.df.index
        
        # ì˜ˆì‹œ ë°ì´í„° ì „ì²˜ë¦¬
        self._preprocess_examples()
    
    def _preprocess_examples(self):
        """ì˜ˆì‹œ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¸ë±ì‹±"""
        self.entire_examples_keyword_no_dash = []
        self.entire_examples_no_dash_with_index = []
        
        for index, examples in enumerate(self.df['examples']):
            if isinstance(examples, dict):
                all_examples = examples.get('correct', []) + examples.get('incorrect', [])
                for example in all_examples:
                    example_no_dash = example.replace('-', '')
                    self.entire_examples_keyword_no_dash.append(example_no_dash)
                    self.entire_examples_no_dash_with_index.append((example_no_dash, index))
        
        # ì˜ˆì‹œ -> ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±
        self.example_to_indices = {}
        for example_no_dash, original_index in self.entire_examples_no_dash_with_index:
            self.example_to_indices.setdefault(example_no_dash, []).append(original_index)
    
    def extract_keywords(self, text, top_k=3):
        """
        ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ)
        
        Args:
            text (str): ì…ë ¥ í…ìŠ¤íŠ¸
            top_k (int): ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            list: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        pos_tags = self.okt.pos(text)
        keywords = [word for word, pos in pos_tags if pos in ['Noun', 'Verb', 'Adjective']]
        return keywords[:top_k]
    
    def hybrid_search(self, keywords, examples, top_k=3):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì™„ì „ì¼ì¹˜, ë¶€ë¶„ì¼ì¹˜, í˜•íƒœì  ìœ ì‚¬ì„± ë“±)
        
        Args:
            keywords (list): ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            examples (list): ê²€ìƒ‰ ëŒ€ìƒ ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            list: ê° í‚¤ì›Œë“œë³„ ë§¤ì¹­ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for keyword in keywords:
            matches = []
            
            for example in examples:
                score = 0
                
                # 1. ì™„ì „ í‚¤ì›Œë“œ ì¼ì¹˜ (ë†’ì€ ì ìˆ˜)
                if keyword == example:
                    score += 100
                elif keyword in example or example in keyword:
                    score += 50
                
                # 2. í˜•íƒœì  ìœ ì‚¬ì„±
                if keyword in example or example in keyword:
                    score += 20
                
                # ê³µí†µ ë¬¸ì ê°œìˆ˜
                common_chars = len(set(keyword) & set(example))
                if common_chars > 0:
                    score += common_chars * 2
                
                # ê¸¸ì´ ìœ ì‚¬ì„±
                len_diff = abs(len(keyword) - len(example))
                if len_diff <= 2:
                    score += (3 - len_diff) * 5
                
                # ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬ ì¼ì¹˜
                if len(keyword) >= 2 and len(example) >= 2:
                    if keyword.startswith(example[:2]) or example.startswith(keyword[:2]):
                        score += 15
                    if keyword.endswith(example[-2:]) or example.endswith(keyword[-2:]):
                        score += 15
                
                if score > 0:
                    matches.append((example, score))
            
            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ìƒìœ„ kê°œ ì„ íƒ
            matches.sort(key=lambda x: x[1], reverse=True)
            results.append(matches[:top_k])
        
        return results
    
    def search(self, sentence, top_k_keywords=3, top_k_matches=10):
        """
        ë¬¸ì¥ì— ëŒ€í•œ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            sentence (str): ê²€ìƒ‰í•  ë¬¸ì¥
            top_k_keywords (int): ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
            top_k_matches (int): ê° í‚¤ì›Œë“œë³„ ë§¤ì¹­ ê°œìˆ˜
            
        Returns:
            dict: ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if self.df is None:
            raise ValueError("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_keywords(sentence, top_k=top_k_keywords)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        results = self.hybrid_search(
            keywords, 
            self.entire_examples_keyword_no_dash, 
            top_k=top_k_matches
        )
        
        # ì¸ë±ìŠ¤ë³„ ìµœê³  ì ìˆ˜ ì§‘ê³„
        index2best = {}
        
        for matches in results:
            for ex, score in matches:
                if score < self.threshold:
                    continue
                
                for original_index in self.example_to_indices.get(ex, []):
                    prev = index2best.get(original_index, -np.inf)
                    if score > prev:
                        index2best[original_index] = score
        
        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_items = sorted(index2best.items(), key=lambda x: x[1], reverse=True)
        retrieved_indices = [idx for idx, sc in sorted_items]
        retrieved_scores = [float(sc) for idx, sc in sorted_items]
        
        return {
            'input_sentence': sentence,
            'keywords': keywords,
            'retrieved_index': retrieved_indices,
            'retrieved_scores': retrieved_scores,
            'total_candidates': len(sorted_items),
            'above_threshold': len([s for s in retrieved_scores if s >= self.threshold])
        }
    
    def analyze_score_distribution(self, results_df):
        """
        ê²€ìƒ‰ ê²°ê³¼ì˜ ì ìˆ˜ ë¶„í¬ ë¶„ì„
        
        Args:
            results_df (pd.DataFrame): ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            dict: ë¶„í¬ í†µê³„
        """
        all_scores = []
        # í†µì¼ëœ ì»¬ëŸ¼ëª… ì‚¬ìš©
        score_column = 'retrieved_scores'
        if score_column in results_df.columns:
            for scores in results_df[score_column]:
                all_scores.extend(scores)
        
        if not all_scores:
            return {"message": "ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        import numpy as np
        
        stats = {
            "total_results": len(all_scores),
            "min_score": float(np.min(all_scores)),
            "max_score": float(np.max(all_scores)),
            "mean_score": float(np.mean(all_scores)),
            "median_score": float(np.median(all_scores)),
            "std_score": float(np.std(all_scores)),
            "percentiles": {
                "25th": float(np.percentile(all_scores, 25)),
                "75th": float(np.percentile(all_scores, 75)),
                "90th": float(np.percentile(all_scores, 90)),
                "95th": float(np.percentile(all_scores, 95))
            },
            "threshold_analysis": {
                "above_70": len([s for s in all_scores if s >= 70]),
                "above_80": len([s for s in all_scores if s >= 80]),
                "above_90": len([s for s in all_scores if s >= 90]),
                "perfect_100": len([s for s in all_scores if s == 100])
            }
        }
        
        return stats

    def batch_search(self, sentences):
        """
        ì—¬ëŸ¬ ë¬¸ì¥ì— ëŒ€í•œ ë°°ì¹˜ ê²€ìƒ‰
        
        Args:
            sentences (list): ê²€ìƒ‰í•  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            pd.DataFrame: ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ (í†µì¼ëœ í˜•íƒœ)
        """
        results = []
        for i, sentence in enumerate(sentences):
            result = self.search(sentence)
            # í†µì¼ëœ í˜•íƒœë¡œ ë³€í™˜
            unified_result = {
                'query_id': i,
                'input_text': sentence,
                'retrieved_indices': result['retrieved_index'],
                'retrieved_scores': result['retrieved_scores'],
                'method': 'method1_keyword_rule_based'
            }
            results.append(unified_result)
        
        df = pd.DataFrame(results)
        
        # ë¶„í¬ ë¶„ì„ ì¶”ê°€
        score_stats = self.analyze_score_distribution(df)
        print("=== Method 1: ì ìˆ˜ ë¶„í¬ ë¶„ì„ ===")
        print(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {score_stats.get('total_results', 0)}ê°œ")
        print(f"ì ìˆ˜ ë²”ìœ„: {score_stats.get('min_score', 0):.1f} ~ {score_stats.get('max_score', 0):.1f}")
        print(f"í‰ê· : {score_stats.get('mean_score', 0):.1f}, ì¤‘ì•™ê°’: {score_stats.get('median_score', 0):.1f}")
        if 'threshold_analysis' in score_stats and isinstance(score_stats['threshold_analysis'], dict):
            threshold_analysis = score_stats['threshold_analysis']
            print(f"ì„ê³„ê°’(70) ì´ìƒ: {threshold_analysis['above_70']}ê°œ")
            print(f"90ì  ì´ìƒ: {threshold_analysis['above_90']}ê°œ")
            print(f"ë§Œì (100): {threshold_analysis['perfect_100']}ê°œ")
        print("=" * 40)
        
        return df
    
    def process_llm_predictions(self, predictions_path):
        """
        LLM ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ '~ì´ ì˜³ë‹¤', '~ê°€ ì˜³ë‹¤' ë¶€ë¶„ ì¶”ì¶œ ë° ê²€ìƒ‰
        
        Args:
            predictions_path (str): LLM ì˜ˆì¸¡ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ
            
        Returns:
            pd.DataFrame: ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
        """
        with open(predictions_path, 'r', encoding='utf-8') as f:
            llm_prediction = json.load(f)
        
        # '~ì´ ì˜³ë‹¤', '~ê°€ ì˜³ë‹¤' ë¶€ë¶„ ì¶”ì¶œ
        processed_sentences = []
        for pred in llm_prediction:
            answer = pred['output']['answer']
            
            if 'ê°€ ì˜³ë‹¤.' in answer:
                input_text = answer.split('ê°€ ì˜³ë‹¤.')[0]
            elif 'ì´ ì˜³ë‹¤.' in answer:
                input_text = answer.split('ì´ ì˜³ë‹¤.')[0]
            else:
                input_text = answer
            
            processed_sentences.append(input_text.strip('"'))
        
        # ë°°ì¹˜ ê²€ìƒ‰ ìˆ˜í–‰
        return self.batch_search(processed_sentences)


def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
    data_paths = {
        'hangeul_rule': 'docs/í•œê¸€ ë§ì¶¤ë²• , í‘œì¤€ì–´  ê·œì •.json',
        'standard_rule': 'docs/í•œê¸€ ë§ì¶¤ë²• , í‘œì¤€ì–´  ê·œì •.json',
        'spacing_rule': 'docs/ë„ì–´ì“°ê¸°.json',
        'punctuation_rule': 'docs/ë¬¸ì¥ ë¶€í˜¸.json',
        'foreign_rule': 'docs/ì™¸ë˜ì–´  í‘œê¸°ë²•.json'
    }
    
    # ê²€ìƒ‰ê¸° ì´ˆê¸°í™” (ì„ê³„ê°’ 70ìœ¼ë¡œ ìœ ì§€)
    searcher = KeywordRuleBasedSearch(data_paths, threshold=70)
    
    # LLM ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬
    results_df = searcher.process_llm_predictions('result/predictions.json')
    
    print("ê²€ìƒ‰ ê²°ê³¼:")
    print(results_df.head())
    
    # ğŸ“ ê²°ê³¼ ì €ì¥ ì¶”ê°€
    output_file = 'result/RAG_result/method1_results.csv'
    results_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
    
    # í†µì¼ëœ í˜•íƒœì˜ CSVë„ ì €ì¥ (êµì§‘í•© ë¶„ì„ìš©)
    try:
        from unified_csv_utils import create_unified_csv
        unified_output = 'result/RAG_result/method1_unified.csv'
        create_unified_csv(results_df, 'method1', unified_output)
    except ImportError:
        print("âš ï¸ unified_csv_utilsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í†µì¼ëœ CSVëŠ” ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    return results_df


if __name__ == "__main__":
    results = main()
