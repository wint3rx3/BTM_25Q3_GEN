"""
Method 3: í•˜ì´ë¸Œë¦¬ë“œ TF-IDF + ìž„ë² ë”© ê²€ìƒ‰
- TF-IDF + Qwen3 ìž„ë² ë”© + Jaccard ìœ ì‚¬ë„ ê²°í•©
- ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ë¸”ë Œë”©
- ë‹¤ë‹¨ê³„ í›„ë³´ ì„ ë³„ ê³¼ì •
- TF-IDFë¡œ í›„ë³´ ì„ ë³„ â†’ ìž„ë² ë”© ìœ ì‚¬ë„ â†’ Jaccard ìœ ì‚¬ë„ â†’ ê°€ì¤‘ì¹˜ ê²°í•©
"""

import ast
import json
import torch
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm


class HybridTFIDFEmbeddingSearch:
    def __init__(self, embedding_model_name='Qwen/Qwen3-Embedding-4B', device=None, score_threshold=0.2):
        """
        í•˜ì´ë¸Œë¦¬ë“œ TF-IDF + ìž„ë² ë”© ê²€ìƒ‰ ì´ˆê¸°í™”
        
        Args:
            embedding_model_name (str): ì‚¬ìš©í•  ìž„ë² ë”© ëª¨ë¸ëª…
            device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
            score_threshold (float): ìµœì¢… ì ìˆ˜ ìž„ê³„ê°’ (ê¸°ë³¸ê°’: 0.2)
        """
        self.embedding_model_name = embedding_model_name
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        self.score_threshold = score_threshold
        
        # ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self._load_embedding_model()
        
        self.df = None
        self.vectorizer = None
        self.tfidf_matrix = None
    
    def _load_embedding_model(self):
        """ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.embedding_model_name, 
            use_fast=True,
            trust_remote_code=True
        )
        self.model = AutoModel.from_pretrained(
            self.embedding_model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            trust_remote_code=True
        )
        self.model.eval()
        
        print(f"ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.embedding_model_name}")
    
    def load_data(self, data_paths):
        """
        ê·œì • ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        
        Args:
            data_paths (dict): ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
        """
        all_data = []
        
        for key, path in data_paths.items():
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_data.extend(data)
        
        self.df = pd.DataFrame(all_data)
        self.df['keywords'] = self.df['keywords'].apply(
            lambda x: ', '.join(x) if isinstance(x, list) else str(x)
        )
        self.df['conditions'] = self.df['conditions'].apply(
            lambda x: ', '.join(x) if isinstance(x, list) else str(x)
        )
        self.df['index'] = self.df.index
        
        # TF-IDF ë²¡í„°í™”
        self._build_tfidf_matrix()
        
        print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ ê·œì •")
    
    def _build_tfidf_matrix(self):
        """TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì¶•"""
        corpus = self.df['keywords'].astype(str).tolist()
        self.vectorizer = TfidfVectorizer(
            ngram_range=(1, 2), 
            min_df=1, 
            max_df=0.9
        )
        self.tfidf_matrix = self.vectorizer.fit_transform(corpus)
        print("TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
    
    @torch.no_grad()
    def qwen3_embed(self, texts, batch_size=32, max_length=2048, l2_normalize=True):
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ Qwen3 ìž„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            texts (list): í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size (int): ë°°ì¹˜ í¬ê¸°
            max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
            l2_normalize (bool): L2 ì •ê·œí™” ì—¬ë¶€
            
        Returns:
            np.ndarray: ìž„ë² ë”© ë²¡í„° ë°°ì—´
        """
        vecs = []
        device = next(self.model.parameters()).device
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            enc = self.tokenizer(
                batch, 
                padding=True, 
                truncation=True,
                max_length=max_length, 
                return_tensors="pt"
            ).to(device)
            
            out = self.model(**enc)
            last_hidden = out.last_hidden_state
            mask = enc["attention_mask"].unsqueeze(-1).to(last_hidden.dtype)
            
            # Mean pooling
            pooled = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)
            v = pooled.detach().float().cpu().numpy()
            
            if l2_normalize:
                n = np.linalg.norm(v, axis=1, keepdims=True)
                n[n == 0] = 1.0
                v = v / n
            
            vecs.append(v)
        
        return np.vstack(vecs)
    
    def compute_jaccard(self, qset, dset):
        """
        ë‘ ì„¸íŠ¸ ê°„ì˜ Jaccard ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            qset (set): ì¿¼ë¦¬ í‚¤ì›Œë“œ ì„¸íŠ¸
            dset (set): ë¬¸ì„œ í‚¤ì›Œë“œ ì„¸íŠ¸
            
        Returns:
            float: Jaccard ìœ ì‚¬ë„
        """
        inter = qset & dset
        union = qset | dset
        return len(inter) / len(union) if union else 0.0
    
    def hybrid_search(self, keywords_list, tfidf_k=50, top_k=10,
                     weight_tfidf=0.5, weight_emb=0.3, weight_jac=0.2):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            keywords_list (list): ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            tfidf_k (int): TF-IDF í›„ë³´ ê°œìˆ˜
            top_k (int): ìµœì¢… ë°˜í™˜ ê²°ê³¼ ê°œìˆ˜
            weight_tfidf (float): TF-IDF ê°€ì¤‘ì¹˜
            weight_emb (float): ìž„ë² ë”© ê°€ì¤‘ì¹˜
            weight_jac (float): Jaccard ê°€ì¤‘ì¹˜
            
        Returns:
            tuple: (ê²€ìƒ‰ëœ DataFrame, ì ìˆ˜ ë¦¬ìŠ¤íŠ¸)
        """
        if self.df is None or self.tfidf_matrix is None:
            raise ValueError("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # 1. TF-IDF ê²€ìƒ‰
        query = " ".join(keywords_list)
        query_vec = self.vectorizer.transform([query])
        tfidf_scores_full = cosine_similarity(query_vec, self.tfidf_matrix)[0]
        
        # í›„ë³´ ì„ ë³„
        num_candidates = min(tfidf_k, len(tfidf_scores_full))
        cand_idx = np.argsort(tfidf_scores_full)[::-1][:num_candidates]
        
        if len(cand_idx) == 0:
            return pd.DataFrame(), []
        
        # TF-IDF ì ìˆ˜ ì •ê·œí™”
        tfidf_scores = tfidf_scores_full[cand_idx]
        tfidf_scores = MinMaxScaler().fit_transform(tfidf_scores.reshape(-1, 1)).flatten()
        
        # 2. ìž„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°
        texts = [self.df.iloc[i]['keywords'] for i in cand_idx]
        db_emb = self.qwen3_embed(texts, batch_size=32)
        query_emb = self.qwen3_embed([query], batch_size=1)[0]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (L2 ì •ê·œí™”í–ˆìœ¼ë¯€ë¡œ ë‚´ì  = ì½”ì‚¬ì¸)
        emb_scores = db_emb @ query_emb
        
        # 3. Jaccard ìœ ì‚¬ë„ ê³„ì‚°
        qset = set(keywords_list)
        jac_scores = np.array([
            self.compute_jaccard(
                qset, 
                set(map(str.strip, self.df.iloc[i]['keywords'].split(',')))
            )
            for i in cand_idx
        ])
        
        # 4. ê°€ì¤‘ì¹˜ ê²°í•©
        final_scores = (
            weight_tfidf * tfidf_scores +
            weight_emb * emb_scores +
            weight_jac * jac_scores
        )
        
        # 5. Top-k ì •ë ¬ ë° ìž„ê³„ê°’ í•„í„°ë§
        # ìž„ê³„ê°’ ì´ìƒë§Œ í•„í„°ë§
        valid_indices = final_scores >= self.score_threshold
        if not np.any(valid_indices):
            return pd.DataFrame(), []
        
        filtered_scores = final_scores[valid_indices]
        filtered_indices = cand_idx[valid_indices]
        
        # Top-k ì„ íƒ
        num_top_results = min(top_k, len(filtered_scores))
        top_positions = np.argsort(filtered_scores)[::-1][:num_top_results]
        
        top_indices = filtered_indices[top_positions]
        top_scores = filtered_scores[top_positions]
        
        return self.df.iloc[top_indices], top_scores.tolist()
    
    def string_list_to_actual_list(self, string_list):
        """
        ë¬¸ìžì—´ í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‹¤ì œ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Args:
            string_list (str or list): ë¬¸ìžì—´ í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            
        Returns:
            list: ë³€í™˜ëœ ë¦¬ìŠ¤íŠ¸
        """
        # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
        if isinstance(string_list, list):
            return string_list
        
        # ë¬¸ìžì—´ì¸ ê²½ìš° íŒŒì‹± ì‹œë„
        try:
            return ast.literal_eval(string_list)
        except (ValueError, SyntaxError):
            cleaned = string_list.strip('[]').strip()
            return [item.strip().strip("'\"") for item in cleaned.split(',')] if cleaned else []
    
    def batch_search(self, keywords_data, tfidf_k=50, top_k=10,
                    weight_tfidf=0.5, weight_emb=0.3, weight_jac=0.2):
        """
        í‚¤ì›Œë“œ ë°ì´í„°ì— ëŒ€í•œ ë°°ì¹˜ ê²€ìƒ‰
        
        Args:
            keywords_data (pd.DataFrame or list): í‚¤ì›Œë“œ ë°ì´í„°
            tfidf_k (int): TF-IDF í›„ë³´ ê°œìˆ˜
            top_k (int): ìµœì¢… ë°˜í™˜ ê²°ê³¼ ê°œìˆ˜
            weight_tfidf (float): TF-IDF ê°€ì¤‘ì¹˜
            weight_emb (float): ìž„ë² ë”© ê°€ì¤‘ì¹˜  
            weight_jac (float): Jaccard ê°€ì¤‘ì¹˜
            
        Returns:
            pd.DataFrame: ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°í”„ë ˆìž„
        """
        index_list_results = []
        score_list_results = []
        
        # keywords_dataê°€ DataFrameì¸ ê²½ìš°
        if isinstance(keywords_data, pd.DataFrame):
            # ì»¬ëŸ¼ëª… ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
            if 'keyword' in keywords_data.columns:
                keyword_column = keywords_data['keyword']
            elif 'keywords' in keywords_data.columns:
                keyword_column = keywords_data['keywords']
            else:
                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©
                keyword_column = keywords_data.iloc[:, 0]
        else:
            keyword_column = keywords_data
        
        for i in tqdm(range(len(keyword_column))):
            if isinstance(keywords_data, pd.DataFrame):
                keywords_list = self.string_list_to_actual_list(keyword_column.iloc[i])
            else:
                keywords_list = keyword_column[i] if isinstance(keyword_column[i], list) else [keyword_column[i]]
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
            top_rows, scores = self.hybrid_search(
                keywords_list, 
                tfidf_k=tfidf_k, 
                top_k=top_k,
                weight_tfidf=weight_tfidf,
                weight_emb=weight_emb,
                weight_jac=weight_jac
            )
            
            # ê²°ê³¼ ì €ìž¥
            top_indices = top_rows['index'].tolist() if not top_rows.empty else []
            index_list_results.append(top_indices)
            score_list_results.append(scores)
        
        # ê²°ê³¼ DataFrame ìƒì„±
        results_df = pd.DataFrame({
            'index_list': index_list_results,
            'score_list': score_list_results
        })
        
        # ì ìˆ˜ ë¶„í¬ ë¶„ì„ ì¶”ê°€
        self._analyze_score_distribution(score_list_results, method_name="Method 3")
        
        return results_df
    
    def _analyze_score_distribution(self, scores_list, method_name="Method 3"):
        """
        ì ìˆ˜ ë¶„í¬ ë¶„ì„ ë° ì¶œë ¥
        
        Args:
            scores_list (list): ê° ì¿¼ë¦¬ë³„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
            method_name (str): ë°©ë²•ëª…
        """
        all_scores = []
        for scores in scores_list:
            all_scores.extend(scores)
        
        if not all_scores:
            print(f"=== {method_name}: ì ìˆ˜ ë°ì´í„° ì—†ìŒ ===")
            return
        
        print(f"=== {method_name}: ì ìˆ˜ ë¶„í¬ ë¶„ì„ ===")
        print(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {len(all_scores)}ê°œ")
        print(f"ì ìˆ˜ ë²”ìœ„: {np.min(all_scores):.3f} ~ {np.max(all_scores):.3f}")
        print(f"í‰ê· : {np.mean(all_scores):.3f}, ì¤‘ì•™ê°’: {np.median(all_scores):.3f}")
        print(f"í‘œì¤€íŽ¸ì°¨: {np.std(all_scores):.3f}")
        print(f"ìž„ê³„ê°’({self.score_threshold}) ì´ìƒ: {len([s for s in all_scores if s >= self.score_threshold])}ê°œ")
        print(f"95th percentile: {np.percentile(all_scores, 95):.3f}")
        print(f"90th percentile: {np.percentile(all_scores, 90):.3f}")
        print(f"75th percentile: {np.percentile(all_scores, 75):.3f}")
        print("=" * 40)
    
    def process_keyword_file(self, keyword_file_path, **search_params):
        """
        í‚¤ì›Œë“œ íŒŒì¼ ì²˜ë¦¬ ë° ê²€ìƒ‰
        
        Args:
            keyword_file_path (str): í‚¤ì›Œë“œ íŒŒì¼ ê²½ë¡œ (JSON ë˜ëŠ” CSV)
            **search_params: ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            
        Returns:
            pd.DataFrame: ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°í”„ë ˆìž„
        """
        # íŒŒì¼ í™•ìž¥ìžì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë¡œë“œ
        if keyword_file_path.endswith('.json'):
            with open(keyword_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # JSONì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords_list = []
            for item in data:
                if 'output' in item and 'keyword' in item['output']:
                    keyword_str = item['output']['keyword']
                    keywords = [k.strip() for k in keyword_str.split(',')]
                    keywords_list.append(keywords)
                else:
                    # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì§ˆë¬¸ì—ì„œ ì¶”ì¶œ
                    question = item['input']['question']
                    keywords_list.append([question])
                    
            # ì»¬ëŸ¼ëª…ì„ 'keyword'ë¡œ í†µì¼í•˜ì—¬ batch_searchì™€ ì¼ì¹˜ì‹œí‚´
            keyword_df = pd.DataFrame({'keyword': keywords_list})
            
        else:
            # CSV íŒŒì¼ ë¡œë“œ
            keyword_df = pd.read_csv(keyword_file_path)
        
        # ë°°ì¹˜ ê²€ìƒ‰ ìˆ˜í–‰
        results_df = self.batch_search(keyword_df, **search_params)
        
        return results_df


def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
    searcher = HybridTFIDFEmbeddingSearch(
        embedding_model_name='Qwen/Qwen3-Embedding-4B',
        score_threshold=0.2
    )
    
    # ë°ì´í„° ë¡œë“œ
    data_paths = {
        'hangeul_rule': 'docs/í•œê¸€ ë§žì¶¤ë²• , í‘œì¤€ì–´  ê·œì •.json',
        'standard_rule': 'docs/í•œê¸€ ë§žì¶¤ë²• , í‘œì¤€ì–´  ê·œì •.json',
        'spacing_rule': 'docs/ë„ì–´ì“°ê¸°.json',
        'punctuation_rule': 'docs/ë¬¸ìž¥ ë¶€í˜¸.json',
        'foreign_rule': 'docs/ì™¸ëž˜ì–´  í‘œê¸°ë²•.json'
    }
    searcher.load_data(data_paths)
    
    # í‚¤ì›Œë“œ íŒŒì¼ ì²˜ë¦¬
    results_df = searcher.process_keyword_file(
        'result/predictions_with_keywords.json',
        tfidf_k=50,
        top_k=10,
        weight_tfidf=0.5,
        weight_emb=0.3,
        weight_jac=0.2
    )
    
    print("ê²€ìƒ‰ ê²°ê³¼:")
    print(results_df.head())
    
    # ðŸ“ ê²°ê³¼ ì €ìž¥ ì¶”ê°€
    output_file = 'result/method3_results.csv'
    results_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ê²°ê³¼ê°€ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
    
    # ìš”ì•½ ì •ë³´ë„ ì €ìž¥
    summary_file = 'result/method3_summary.txt'
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=== Method 3: í•˜ì´ë¸Œë¦¬ë“œ TF-IDF + ìž„ë² ë”© ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ===\n")
        f.write(f"ì´ ì¿¼ë¦¬ ìˆ˜: {len(results_df)}\n")
        
        # ê²°ê³¼ê°€ ìžˆëŠ” ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
        results_with_data = len([r for r in results_df['index_list'] if r])
        f.write(f"ê²°ê³¼ê°€ ìžˆëŠ” ì¿¼ë¦¬: {results_with_data}\n")
        
        # ì ìˆ˜ í†µê³„ ê³„ì‚°
        all_scores = []
        for scores in results_df['score_list']:
            all_scores.extend(scores)
        
        if all_scores:
            import numpy as np
            f.write(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {len(all_scores)}ê°œ\n")
            f.write(f"í‰ê·  ì ìˆ˜: {np.mean(all_scores):.3f}\n")
            f.write(f"ì¤‘ì•™ê°’ ì ìˆ˜: {np.median(all_scores):.3f}\n")
            f.write(f"ìµœê³  ì ìˆ˜: {np.max(all_scores):.3f}\n")
            f.write(f"ìµœì € ì ìˆ˜: {np.min(all_scores):.3f}\n")
            f.write(f"í‘œì¤€íŽ¸ì°¨: {np.std(all_scores):.3f}\n")
            f.write(f"ìž„ê³„ê°’({searcher.score_threshold}) ì´ìƒ: {len([s for s in all_scores if s >= searcher.score_threshold])}ê°œ\n")
            f.write(f"95th percentile: {np.percentile(all_scores, 95):.3f}\n")
            f.write(f"90th percentile: {np.percentile(all_scores, 90):.3f}\n")
            f.write(f"75th percentile: {np.percentile(all_scores, 75):.3f}\n")
            
            # ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
            f.write(f"\nì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬:\n")
            f.write(f"0.8 ì´ìƒ: {len([s for s in all_scores if s >= 0.8])}ê°œ\n")
            f.write(f"0.6-0.8: {len([s for s in all_scores if 0.6 <= s < 0.8])}ê°œ\n")
            f.write(f"0.4-0.6: {len([s for s in all_scores if 0.4 <= s < 0.6])}ê°œ\n")
            f.write(f"0.2-0.4: {len([s for s in all_scores if 0.2 <= s < 0.4])}ê°œ\n")
        else:
            f.write("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\n")
    
    print(f"âœ… ìš”ì•½ ì •ë³´ê°€ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤: {summary_file}")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • ì •ë³´ë„ ì €ìž¥
    config_file = 'result/method3_config.txt'
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write("=== Method 3: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • ì •ë³´ ===\n")
        f.write(f"ìž„ë² ë”© ëª¨ë¸: {searcher.embedding_model_name}\n")
        f.write(f"ë””ë°”ì´ìŠ¤: {searcher.device}\n")
        f.write(f"ì ìˆ˜ ìž„ê³„ê°’: {searcher.score_threshold}\n")
        f.write(f"\nê²€ìƒ‰ íŒŒë¼ë¯¸í„°:\n")
        f.write(f"TF-IDF í›„ë³´ ìˆ˜ (tfidf_k): 50\n")
        f.write(f"ìµœì¢… ê²°ê³¼ ìˆ˜ (top_k): 10\n")
        f.write(f"\nê°€ì¤‘ì¹˜ ì„¤ì •:\n")
        f.write(f"TF-IDF ê°€ì¤‘ì¹˜: 0.5\n")
        f.write(f"ìž„ë² ë”© ê°€ì¤‘ì¹˜: 0.3\n")
        f.write(f"Jaccard ê°€ì¤‘ì¹˜: 0.2\n")
        f.write(f"\në°ì´í„° ì •ë³´:\n")
        f.write(f"ê·œì • ë°ì´í„° ìˆ˜: {len(searcher.df)}ê°œ\n")
        f.write(f"TF-IDF ë²¡í„° ì°¨ì›: {searcher.tfidf_matrix.shape}\n")
        
        # ìž…ë ¥ íŒŒì¼ ì •ë³´
        import os
        input_file = 'result/predictions_with_keywords.json'
        if os.path.exists(input_file):
            f.write(f"ìž…ë ¥ íŒŒì¼: {input_file}\n")
            f.write(f"ìž…ë ¥ íŒŒì¼ í¬ê¸°: {os.path.getsize(input_file)} bytes\n")
    
    print(f"âœ… ê²€ìƒ‰ ì„¤ì • ì •ë³´ê°€ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤: {config_file}")
    
    # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥
    print(f"\nðŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
    for i in range(min(3, len(results_df))):
        indices = results_df.iloc[i]['index_list']
        scores = results_df.iloc[i]['score_list']
        print(f"Query {i+1}: {len(indices)}ê°œ ê²°ê³¼, ìµœê³  ì ìˆ˜: {max(scores):.3f}" if scores else f"Query {i+1}: ê²°ê³¼ ì—†ìŒ")
    
    return results_df


if __name__ == "__main__":
    results = main()
