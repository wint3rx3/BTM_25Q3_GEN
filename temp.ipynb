{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a147c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ predictions.json ë¡œë”© ì¤‘...\n",
      "ğŸ“„ extracted_keywords.json ë¡œë”© ì¤‘...\n",
      "ğŸ“Š ì´ 498ê°œì˜ predictions ë°ì´í„°\n",
      "ğŸ“Š ì´ 498ê°œì˜ keywords ë°ì´í„°\n",
      "âœ… í•©ì³ì§„ ê²°ê³¼ê°€ result/predictions_with_keywords.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "\n",
      "ğŸ“‹ ì²« ë²ˆì§¸ í•­ëª©ì˜ êµ¬ì¡°:\n",
      "{\n",
      "  \"id\": \"750\",\n",
      "  \"input\": {\n",
      "    \"question_type\": \"êµì •í˜•\",\n",
      "    \"question\": \"ë‹¤ìŒ ì¤‘ ì–´ë¬¸ ê·œë²”ì— ë¶€í•©í•˜ëŠ” ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\\nâ€• ë‹¤ì‹œëŠ” ê²°í˜¼í•˜ê³  ì‹¶ì§€ ì•Šì•„ìš”, \\nâ€• ë‹¤ì‹œëŠ” ê²°í˜¼í•˜ê³  ì‹¶ì§€ ì•Šì•„ìš”. \"\n",
      "  },\n",
      "  \"output\": {\n",
      "    \"answer\": \"\\\"ë‹¤ì‹œëŠ” ê²°í˜¼í•˜ê³  ì‹¶ì§€ ì•Šì•„ìš”.\\\"ê°€ ì˜³ë‹¤. ì‰¬í”„íŠ¸(Shift)ì™€ íƒ­(Tab)ì„ ì´ìš©í•œ ê³µë°±ì˜ ì‚¬ìš©ì€ í‘œì¤€ì–´ ì‚¬ìš©ì— ì í•©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.\",\n",
      "    \"keyword\": \"ì‰¬í”„íŠ¸, íƒ­, í‘œì¤€ì–´\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def merge_predictions_with_keywords():\n",
    "    \"\"\"predictions.jsonê³¼ extracted_keywords.jsonì„ í•©ì³ì„œ ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # predictions.json ë¡œë“œ\n",
    "    print(\"ğŸ“„ predictions.json ë¡œë”© ì¤‘...\")\n",
    "    with open(\"result/predictions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    # extracted_keywords.json ë¡œë“œ\n",
    "    print(\"ğŸ“„ extracted_keywords.json ë¡œë”© ì¤‘...\")\n",
    "    with open(\"result/extracted_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        keywords_data = json.load(f)\n",
    "    \n",
    "    # í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ idë¥¼ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "    keywords_dict = {}\n",
    "    for item in keywords_data:\n",
    "        keywords_dict[item[\"id\"]] = item[\"keywords\"]\n",
    "    \n",
    "    print(f\"ğŸ“Š ì´ {len(predictions)}ê°œì˜ predictions ë°ì´í„°\")\n",
    "    print(f\"ğŸ“Š ì´ {len(keywords_dict)}ê°œì˜ keywords ë°ì´í„°\")\n",
    "    \n",
    "    # í•©ì¹œ ê²°ê³¼ ìƒì„±\n",
    "    merged_results = []\n",
    "    \n",
    "    for item in predictions:\n",
    "        item_id = item[\"id\"]\n",
    "        \n",
    "        # ê¸°ì¡´ êµ¬ì¡° ìœ ì§€í•˜ë©´ì„œ outputì— keyword ì¶”ê°€\n",
    "        merged_item = {\n",
    "            \"id\": item_id,\n",
    "            \"input\": item[\"input\"],\n",
    "            \"output\": {\n",
    "                \"answer\": item[\"output\"][\"answer\"],\n",
    "                \"keyword\": \", \".join(keywords_dict.get(item_id, [])) if item_id in keywords_dict else \"\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        merged_results.append(merged_item)\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    output_file = \"result/predictions_with_keywords.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… í•©ì³ì§„ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ í•­ëª© êµ¬ì¡° í™•ì¸\n",
    "    if merged_results:\n",
    "        print(\"\\nğŸ“‹ ì²« ë²ˆì§¸ í•­ëª©ì˜ êµ¬ì¡°:\")\n",
    "        print(json.dumps(merged_results[0], ensure_ascii=False, indent=2))\n",
    "    \n",
    "    return merged_results\n",
    "\n",
    "# ì‹¤í–‰\n",
    "merged_data = merge_predictions_with_keywords()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
