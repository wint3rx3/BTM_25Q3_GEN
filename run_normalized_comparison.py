"""
í†µì¼ëœ ì ìˆ˜ ì²´ê³„(0~1)ë¡œ ì„¸ ë°©ë²• ë¹„êµ
- Method 1: ì •ê·œí™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (0~1)
- Method 2: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0~1) 
- Method 3: ê°€ì¤‘ì¹˜ ê²°í•© ì ìˆ˜ (0~1)
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from dotenv import load_dotenv
from method1_keyword_rule_based_search import KeywordRuleBasedSearch
from method2_semantic_embedding_search import SemanticEmbeddingSearch
from method3_hybrid_tfidf_embedding_search import HybridTFIDFEmbeddingSearch

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

def run_all_methods():
    """ì„¸ ë°©ë²• ëª¨ë‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ë¹„êµ"""
    
    # ê³µí†µ ë°ì´í„° ê²½ë¡œ
    data_paths = {
        'hangeul_rule': 'docs/í•œê¸€ ë§ì¶¤ë²• , í‘œì¤€ì–´  ê·œì •.json',
        'standard_rule': 'docs/í•œê¸€ ë§ì¶¤ë²• , í‘œì¤€ì–´  ê·œì •.json',
        'spacing_rule': 'docs/ë„ì–´ì“°ê¸°.json',
        'punctuation_rule': 'docs/ë¬¸ì¥ ë¶€í˜¸.json',
        'foreign_rule': 'docs/ì™¸ë˜ì–´  í‘œê¸°ë²•.json'
    }
    
    print("ğŸš€ í†µì¼ëœ ì ìˆ˜ ì²´ê³„(0~1)ë¡œ ì„¸ ë°©ë²• ì‹¤í–‰ ì‹œì‘...")
    print("=" * 60)
    
    # Method 1: ì •ê·œí™”ëœ ë£°ë² ì´ìŠ¤ ê²€ìƒ‰
    print("ğŸ“Œ Method 1: ì •ê·œí™”ëœ í‚¤ì›Œë“œ ë£°ë² ì´ìŠ¤ ê²€ìƒ‰")
    method1 = KeywordRuleBasedSearch(data_paths, threshold=0.7)  # 0.7ë¡œ ë³€ê²½
    results1 = method1.process_llm_predictions('result/predictions.json')
    results1.to_csv('method1_normalized_results.csv', index=False)
    print("âœ… Method 1 ì™„ë£Œ\n")
    
    # Method 2: ì˜ë¯¸ ê¸°ë°˜ ì„ë² ë”© ê²€ìƒ‰
    print("ğŸ“Œ Method 2: ì˜ë¯¸ ê¸°ë°˜ ì„ë² ë”© ê²€ìƒ‰")
    
    # .envì—ì„œ HF_TOKEN ê°€ì ¸ì˜¤ê¸°
    hf_token = os.getenv('HF_TOKEN')
    if not hf_token:
        print("âš ï¸ ê²½ê³ : .env íŒŒì¼ì— HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    method2 = SemanticEmbeddingSearch(
        model_name='Qwen/Qwen3-Embedding-4B',
        hf_token=hf_token,
        score_threshold=0.3
    )
    method2.load_data(data_paths)
    vectordb_dir = "./vectordb_rules"
    method2.build_faiss_index(vectordb_dir)
    results2 = method2.process_llm_predictions('result/predictions.json', vectordb_dir, top_k=10)
    results2.to_csv('method2_results_top10.csv', index=False)
    print("âœ… Method 2 ì™„ë£Œ\n")
    
    # Method 3: í•˜ì´ë¸Œë¦¬ë“œ TF-IDF + ì„ë² ë”© ê²€ìƒ‰
    print("ğŸ“Œ Method 3: í•˜ì´ë¸Œë¦¬ë“œ TF-IDF + ì„ë² ë”© ê²€ìƒ‰")
    method3 = HybridTFIDFEmbeddingSearch(
        embedding_model_name='Qwen/Qwen3-Embedding-4B',
        score_threshold=0.2
    )
    method3.load_data(data_paths)
    results3 = method3.process_keyword_file(
        'result/predictions_with_keywords.json',
        tfidf_k=50, top_k=10
    )
    results3.to_csv('method3_results_top10.csv', index=False)
    print("âœ… Method 3 ì™„ë£Œ\n")
    
    print("ğŸ¯ ëª¨ë“  ë°©ë²• ì‹¤í–‰ ì™„ë£Œ!")
    return results1, results2, results3

def analyze_unified_scores(results1, results2, results3):
    """í†µì¼ëœ ì ìˆ˜ ì²´ê³„ë¡œ ë¶„ì„"""
    
    print("\nğŸ“Š í†µì¼ëœ ì ìˆ˜ ì²´ê³„ ë¶„ì„ (0~1 ë²”ìœ„)")
    print("=" * 50)
    
    # Method 1 ì ìˆ˜ ì¶”ì¶œ
    method1_scores = []
    for scores in results1['retrieved_scores']:
        method1_scores.extend(scores)
    
    # Method 2 ì ìˆ˜ ì¶”ì¶œ  
    method2_scores = []
    for scores in results2['top_10_scores']:
        method2_scores.extend(scores)
    
    # Method 3 ì ìˆ˜ ì¶”ì¶œ
    method3_scores = []
    for scores in results3['score_list']:
        method3_scores.extend(scores)
    
    # ë¹„êµ í‘œ ìƒì„±
    comparison_data = {
        'Method': ['Method 1 (ì •ê·œí™”)', 'Method 2 (ì„ë² ë”©)', 'Method 3 (í•˜ì´ë¸Œë¦¬ë“œ)'],
        'Total Results': [len(method1_scores), len(method2_scores), len(method3_scores)],
        'Min Score': [f"{np.min(method1_scores):.3f}" if method1_scores else "N/A",
                     f"{np.min(method2_scores):.3f}" if method2_scores else "N/A", 
                     f"{np.min(method3_scores):.3f}" if method3_scores else "N/A"],
        'Max Score': [f"{np.max(method1_scores):.3f}" if method1_scores else "N/A",
                     f"{np.max(method2_scores):.3f}" if method2_scores else "N/A",
                     f"{np.max(method3_scores):.3f}" if method3_scores else "N/A"],
        'Mean': [f"{np.mean(method1_scores):.3f}" if method1_scores else "N/A",
                f"{np.mean(method2_scores):.3f}" if method2_scores else "N/A",
                f"{np.mean(method3_scores):.3f}" if method3_scores else "N/A"],
        'Std': [f"{np.std(method1_scores):.3f}" if method1_scores else "N/A",
               f"{np.std(method2_scores):.3f}" if method2_scores else "N/A", 
               f"{np.std(method3_scores):.3f}" if method3_scores else "N/A"],
        'Above 0.7': [len([s for s in method1_scores if s >= 0.7]),
                     len([s for s in method2_scores if s >= 0.7]),
                     len([s for s in method3_scores if s >= 0.7])],
        'Above 0.9': [len([s for s in method1_scores if s >= 0.9]),
                     len([s for s in method2_scores if s >= 0.9]),
                     len([s for s in method3_scores if s >= 0.9])]
    }
    
    df_comparison = pd.DataFrame(comparison_data)
    print(df_comparison.to_string(index=False))
    
    # ì‹œê°í™”
    plt.figure(figsize=(15, 5))
    
    # íˆìŠ¤í† ê·¸ë¨
    plt.subplot(1, 3, 1)
    if method1_scores:
        plt.hist(method1_scores, bins=20, alpha=0.7, label='Method 1', color='blue')
    if method2_scores:
        plt.hist(method2_scores, bins=20, alpha=0.7, label='Method 2', color='red')
    if method3_scores:
        plt.hist(method3_scores, bins=20, alpha=0.7, label='Method 3', color='green')
    plt.xlabel('Score (0~1)')
    plt.ylabel('Frequency')
    plt.title('Score Distribution Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ë°•ìŠ¤í”Œë¡¯
    plt.subplot(1, 3, 2)
    all_scores = []
    labels = []
    if method1_scores:
        all_scores.append(method1_scores)
        labels.append('Method 1')
    if method2_scores:
        all_scores.append(method2_scores)
        labels.append('Method 2')
    if method3_scores:
        all_scores.append(method3_scores)
        labels.append('Method 3')
    
    if all_scores:
        plt.boxplot(all_scores, labels=labels)
        plt.ylabel('Score (0~1)')
        plt.title('Score Distribution (Box Plot)')
        plt.grid(True, alpha=0.3)
    
    # ì„ê³„ê°’ë³„ ë¹„êµ
    plt.subplot(1, 3, 3)
    thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]
    method1_counts = [len([s for s in method1_scores if s >= t]) for t in thresholds] if method1_scores else [0]*5
    method2_counts = [len([s for s in method2_scores if s >= t]) for t in thresholds] if method2_scores else [0]*5
    method3_counts = [len([s for s in method3_scores if s >= t]) for t in thresholds] if method3_scores else [0]*5
    
    x = np.arange(len(thresholds))
    width = 0.25
    
    plt.bar(x - width, method1_counts, width, label='Method 1', color='blue', alpha=0.7)
    plt.bar(x, method2_counts, width, label='Method 2', color='red', alpha=0.7)
    plt.bar(x + width, method3_counts, width, label='Method 3', color='green', alpha=0.7)
    
    plt.xlabel('Threshold')
    plt.ylabel('Count Above Threshold')
    plt.title('Results Above Thresholds')
    plt.xticks(x, [f'â‰¥{t}' for t in thresholds])
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('unified_score_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ë¹„êµ ê²°ê³¼ ì €ì¥
    df_comparison.to_csv('unified_score_comparison.csv', index=False)
    
    return df_comparison

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ëª¨ë“  ë°©ë²• ì‹¤í–‰
        results1, results2, results3 = run_all_methods()
        
        # í†µì¼ëœ ì ìˆ˜ ë¶„ì„
        comparison_df = analyze_unified_scores(results1, results2, results3)
        
        print("\nğŸ‰ ë¶„ì„ ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼:")
        print("- method1_normalized_results.csv")
        print("- method2_results_top10.csv") 
        print("- method3_results_top10.csv")
        print("- unified_score_comparison.csv")
        print("- unified_score_comparison.png")
        
        return comparison_df
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

if __name__ == "__main__":
    comparison_result = main()