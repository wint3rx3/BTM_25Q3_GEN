"""
Method 2: ì˜ë¯¸ ê¸°ë°˜ ì„ë² ë”© ê²€ìƒ‰
- Qwen3-Embedding-4B ëª¨ë¸ ì‚¬ìš©
- êµì • ì‚¬ìœ  â†” ê·œì •(rule + conditions) ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
- FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í™œìš©
- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ Kê°œ ê²€ìƒ‰
"""

import os
import json
import torch
import faiss
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModel
from huggingface_hub import login
from dotenv import load_dotenv
import re

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class SemanticEmbeddingSearch:
    def __init__(self, model_name='Qwen/Qwen3-Embedding-4B', device=None, hf_token=None, score_threshold=0.3):
        """
        ì˜ë¯¸ ê¸°ë°˜ ì„ë² ë”© ê²€ìƒ‰ ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…
            device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=ìë™ì„ íƒ)
            hf_token (str): Hugging Face í† í°
            score_threshold (float): ìœ ì‚¬ë„ ì ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)
        """
        self.model_name = model_name
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        self.score_threshold = score_threshold
        
        # Hugging Face ë¡œê·¸ì¸
        if hf_token:
            login(hf_token)
        
        # ëª¨ë¸ ë¡œë“œ
        self.tokenizer, self.model, self.device = self._load_model()
        self.df = None
        self.vectordb_dir = None
    
    def _load_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
        model = AutoModel.from_pretrained(
            self.model_name, 
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            trust_remote_code=True
        ).eval()
        
        model = model.to(self.device)
        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_name} on {self.device}")
        
        return tokenizer, model, self.device
    
    def load_data(self, data_paths):
        """
        ê·œì • ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        
        Args:
            data_paths (dict): ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
        """
        all_data = []
        
        for key, path in data_paths.items():
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_data.extend(data)
        
        self.df = pd.DataFrame(all_data)
        self.df['keywords'] = self.df['keywords'].apply(
            lambda x: ', '.join(x) if isinstance(x, list) else str(x)
        )
        self.df['conditions'] = self.df['conditions'].apply(
            lambda x: ', '.join(x) if isinstance(x, list) else str(x)
        )
        self.df['index'] = self.df.index
        
        print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ ê·œì •")
    
    def _to_text(self, rule, conditions):
        """
        ê·œì •ê³¼ ì¡°ê±´ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        
        Args:
            rule: ê·œì • í…ìŠ¤íŠ¸
            conditions: ì¡°ê±´ í…ìŠ¤íŠ¸
            
        Returns:
            str: ê²°í•©ëœ í…ìŠ¤íŠ¸
        """
        def normalize(x):
            if isinstance(x, list):
                return " ".join(map(str, x))
            return "" if pd.isna(x) else str(x)
        
        r = normalize(rule)
        c = normalize(conditions)
        return (r + ("\n" + c if c else "")).strip()
    
    @torch.no_grad()
    def encode_texts(self, texts, batch_size=32, normalize=True):
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            texts (list): í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size (int): ë°°ì¹˜ í¬ê¸°
            normalize (bool): L2 ì •ê·œí™” ì—¬ë¶€
            
        Returns:
            np.ndarray: ì„ë² ë”© ë²¡í„° ë°°ì—´
        """
        vecs = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            inputs = self.tokenizer(
                batch, 
                padding=True, 
                truncation=True, 
                return_tensors="pt", 
                max_length=2048
            ).to(self.device)
            
            outputs = self.model(**inputs)
            last_hidden = outputs.last_hidden_state  # [B, T, H]
            attn_mask = inputs['attention_mask'].unsqueeze(-1)  # [B, T, 1]
            
            # Mean pooling
            summed = (last_hidden * attn_mask).sum(dim=1)
            counts = attn_mask.sum(dim=1).clamp(min=1)
            emb = (summed / counts).detach().float().cpu().numpy()  # [B, H]
            vecs.append(emb)
        
        X = np.vstack(vecs).astype('float32')
        if normalize:
            faiss.normalize_L2(X)  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‚´ì 
        return X
    
    def encode_query(self, query):
        """
        ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            query (str): ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            
        Returns:
            np.ndarray: ì„ë² ë”© ë²¡í„°
        """
        return self.encode_texts([query], batch_size=1, normalize=True)[0]
    
    def build_faiss_index(self, vectordb_dir):
        """
        FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ë° ì €ì¥
        
        Args:
            vectordb_dir (str): ë²¡í„° DB ì €ì¥ ë””ë ‰í† ë¦¬
        """
        if self.df is None:
            raise ValueError("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        os.makedirs(vectordb_dir, exist_ok=True)
        self.vectordb_dir = vectordb_dir
        
        # ë©”íƒ€ë°ì´í„° ë° ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ ìƒì„±
        docs = []
        texts = []
        
        for _, row in self.df.iterrows():
            text = self._to_text(row.get('rule', ''), row.get('conditions', ''))
            docs.append({
                "index": int(row['index']),
                "rule": row.get('rule', ''),
                "conditions": row.get('conditions', ''),
                "text": text
            })
            texts.append(text)
        
        print("ì„ë² ë”© ê³„ì‚° ì¤‘...")
        # ì„ë² ë”© ê³„ì‚°
        X = self.encode_texts(texts, batch_size=64, normalize=True)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‚´ì )
        d = X.shape[1]
        index = faiss.IndexFlatIP(d)
        index.add(X)
        
        # ì €ì¥
        faiss.write_index(index, os.path.join(vectordb_dir, "index.faiss"))
        with open(os.path.join(vectordb_dir, "docs.jsonl"), "w", encoding="utf-8") as f:
            for doc in docs:
                f.write(json.dumps(doc, ensure_ascii=False) + "\n")
        
        print(f"FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {vectordb_dir}")
    
    def load_faiss_index(self, vectordb_dir):
        """
        ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        
        Args:
            vectordb_dir (str): ë²¡í„° DB ë””ë ‰í† ë¦¬
            
        Returns:
            tuple: (FAISS ì¸ë±ìŠ¤, ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸)
        """
        index = faiss.read_index(os.path.join(vectordb_dir, "index.faiss"))
        docs = []
        with open(os.path.join(vectordb_dir, "docs.jsonl"), "r", encoding="utf-8") as f:
            for line in f:
                docs.append(json.loads(line))
        return index, docs
    
    def search(self, query, vectordb_dir=None, top_k=10):
        """
        ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ ê²€ìƒ‰
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            vectordb_dir (str): ë²¡í„° DB ë””ë ‰í† ë¦¬ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            list: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if vectordb_dir is None:
            vectordb_dir = self.vectordb_dir
        
        if vectordb_dir is None:
            raise ValueError("vectordb_dirì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        index, docs = self.load_faiss_index(vectordb_dir)
        
        # ì¿¼ë¦¬ ì„ë² ë”© ë° ê²€ìƒ‰
        q = self.encode_query(query).reshape(1, -1).astype('float32')
        D, I = index.search(q, top_k)
        
        results = []
        for rank, (idx, score) in enumerate(zip(I[0], D[0])):
            if idx == -1:
                continue
            meta = docs[idx]
            results.append({
                "rank": rank + 1,
                "score": float(score),
                "rule_index": meta["index"],
                "rule": meta["rule"],
                "conditions": meta["conditions"]
            })
        
        return results
    
    def batch_search(self, queries, vectordb_dir=None, top_k=10):
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ì— ëŒ€í•œ ë°°ì¹˜ ê²€ìƒ‰
        
        Args:
            queries (list): ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            vectordb_dir (str): ë²¡í„° DB ë””ë ‰í† ë¦¬
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            pd.DataFrame: ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
        """
        if vectordb_dir is None:
            vectordb_dir = self.vectordb_dir
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        index, docs = self.load_faiss_index(vectordb_dir)
        
        # ë°°ì¹˜ ì„ë² ë”©
        Q = self.encode_texts(queries, batch_size=64, normalize=True)
        
        # ë°°ì¹˜ ê²€ìƒ‰
        D, I = index.search(Q, top_k)
        
        top_k_idx_list = []
        top_k_scores_list = []
        
        for scores, idxs in zip(D, I):
            # ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤(-1) ì œì™¸ ë° ì„ê³„ê°’ í•„í„°ë§
            filtered = [(i, s) for i, s in zip(idxs, scores) if i != -1 and s >= self.score_threshold]
            
            if not filtered:
                top_k_idx_list.append([])
                top_k_scores_list.append([])
                continue
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì›ë³¸ rule ì¸ë±ìŠ¤ ì¶”ì¶œ
            rule_indices = [docs[i]["index"] for i, _ in filtered]
            rule_scores = [float(s) for _, s in filtered]
            
            top_k_idx_list.append(rule_indices)
            top_k_scores_list.append(rule_scores)
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        results_df = pd.DataFrame()
        results_df[f'top_{top_k}_rule_index'] = top_k_idx_list
        results_df[f'top_{top_k}_scores'] = top_k_scores_list
        
        # ì ìˆ˜ ë¶„í¬ ë¶„ì„ ì¶”ê°€
        self._analyze_score_distribution(top_k_scores_list, method_name="Method 2")
        
        return results_df
    
    def _analyze_score_distribution(self, scores_list, method_name="Method 2"):
        """
        ì ìˆ˜ ë¶„í¬ ë¶„ì„ ë° ì¶œë ¥
        
        Args:
            scores_list (list): ê° ì¿¼ë¦¬ë³„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
            method_name (str): ë°©ë²•ëª…
        """
        all_scores = []
        for scores in scores_list:
            all_scores.extend(scores)
        
        if not all_scores:
            print(f"=== {method_name}: ì ìˆ˜ ë°ì´í„° ì—†ìŒ ===")
            return
        
        import numpy as np
        
        print(f"=== {method_name}: ì ìˆ˜ ë¶„í¬ ë¶„ì„ ===")
        print(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {len(all_scores)}ê°œ")
        print(f"ì ìˆ˜ ë²”ìœ„: {np.min(all_scores):.3f} ~ {np.max(all_scores):.3f}")
        print(f"í‰ê· : {np.mean(all_scores):.3f}, ì¤‘ì•™ê°’: {np.median(all_scores):.3f}")
        print(f"í‘œì¤€í¸ì°¨: {np.std(all_scores):.3f}")
        print(f"ì„ê³„ê°’({self.score_threshold}) ì´ìƒ: {len([s for s in all_scores if s >= self.score_threshold])}ê°œ")
        print(f"95th percentile: {np.percentile(all_scores, 95):.3f}")
        print(f"90th percentile: {np.percentile(all_scores, 90):.3f}")
        print(f"75th percentile: {np.percentile(all_scores, 75):.3f}")
        print("=" * 40)
    
    def process_llm_predictions(self, predictions_path, vectordb_dir=None, top_k=10):
        """
        LLM ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ êµì • ì‚¬ìœ  ì¶”ì¶œ ë° ê²€ìƒ‰
        
        Args:
            predictions_path (str): LLM ì˜ˆì¸¡ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ
            vectordb_dir (str): ë²¡í„° DB ë””ë ‰í† ë¦¬
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            pd.DataFrame: ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
        """
        with open(predictions_path, 'r', encoding='utf-8') as f:
            llm_prediction = json.load(f)
        
        # êµì • ì‚¬ìœ  ì¶”ì¶œ
        id_list = []
        question_list = []
        reason_list = []
        
        for pred in llm_prediction:
            id_val = pred['id']
            question = pred['input']['question']
            ans = pred['output']['answer']
            
            # 'ì˜³ë‹¤' ë¶€ë¶„ ì¶”ì¶œ
            pat = r'(?:^|(?<=[.!?â€¦ã€‚ï¼ï¼Ÿ]))\s*(?:(?:"[^"]*"|\'[^\']*\'|["][^"]*["]|['][^']*[']))?[^.!?â€¦ã€‚ï¼ï¼Ÿ]*ì˜³ë‹¤[^.!?â€¦ã€‚ï¼ï¼Ÿ]*[.!?â€¦ã€‚ï¼ï¼Ÿ]'
            m = re.search(pat, ans)
            if m:
                answer = m.group(0)
                # 'ì˜³ë‹¤.' ì´í›„ ì‚¬ìœ  ì¶”ì¶œ
                m_reason = re.search(r'(?<=ì˜³ë‹¤\.)\s*(.*)$', ans, flags=re.S)
                reason = m_reason.group(1) if m_reason else ""
            else:
                reason = ans
            
            id_list.append(id_val)
            question_list.append(question)
            reason_list.append(reason)
        
        # ì‚¬ìœ  DataFrame ìƒì„±
        df_reason = pd.DataFrame({
            'id': id_list,
            'question': question_list,
            'reason': reason_list
        })
        
        # ë°°ì¹˜ ê²€ìƒ‰ ìˆ˜í–‰
        search_results = self.batch_search(
            df_reason['reason'].fillna("").astype(str).tolist(),
            vectordb_dir,
            top_k
        )
        
        # ê²°ê³¼ ê²°í•©
        result_df = pd.concat([df_reason, search_results], axis=1)
        
        return result_df


def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    # ê²€ìƒ‰ê¸° ì´ˆê¸°í™” (.envì—ì„œ HF_TOKEN ë¡œë“œ)
    hf_token = os.getenv('HF_TOKEN')
    if not hf_token:
        print("âš ï¸ ê²½ê³ : .env íŒŒì¼ì— HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    searcher = SemanticEmbeddingSearch(
        model_name='Qwen/Qwen3-Embedding-4B',
        hf_token=hf_token
    )
    
    # ë°ì´í„° ë¡œë“œ
    data_paths = {
        'hangeul_rule': 'docs/í•œê¸€ ë§ì¶¤ë²• , í‘œì¤€ì–´  ê·œì •.json',
        'standard_rule': 'docs/í•œê¸€ ë§ì¶¤ë²• , í‘œì¤€ì–´  ê·œì •.json',
        'spacing_rule': 'docs/ë„ì–´ì“°ê¸°.json',
        'punctuation_rule': 'docs/ë¬¸ì¥ ë¶€í˜¸.json',
        'foreign_rule': 'docs/ì™¸ë˜ì–´  í‘œê¸°ë²•.json'
    }
    searcher.load_data(data_paths)
    
    # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
    vectordb_dir = "./vectordb_rules"
    searcher.build_faiss_index(vectordb_dir)
    
    # LLM ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬
    results_df = searcher.process_llm_predictions(
        'result/predictions.json',
        vectordb_dir,
        top_k=10
    )
    
    print("ê²€ìƒ‰ ê²°ê³¼:")
    print(results_df.head())
    
    # ğŸ“ ê²°ê³¼ ì €ì¥ ì¶”ê°€
    output_file = 'result/method2_results.csv'
    results_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
    
    # ìš”ì•½ ì •ë³´ë„ ì €ì¥
    summary_file = 'result/method2_summary.txt'
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=== Method 2: ì˜ë¯¸ ê¸°ë°˜ ì„ë² ë”© ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ===\n")
        f.write(f"ì´ ì¿¼ë¦¬ ìˆ˜: {len(results_df)}\n")
        
        # ê²°ê³¼ê°€ ìˆëŠ” ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
        results_with_data = len([r for r in results_df['top_10_rule_index'] if r])
        f.write(f"ê²°ê³¼ê°€ ìˆëŠ” ì¿¼ë¦¬: {results_with_data}\n")
        
        # ì ìˆ˜ í†µê³„ ê³„ì‚°
        all_scores = []
        for scores in results_df['top_10_scores']:
            all_scores.extend(scores)
        
        if all_scores:
            import numpy as np
            f.write(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {len(all_scores)}ê°œ\n")
            f.write(f"í‰ê·  ì ìˆ˜: {np.mean(all_scores):.3f}\n")
            f.write(f"ì¤‘ì•™ê°’ ì ìˆ˜: {np.median(all_scores):.3f}\n")
            f.write(f"ìµœê³  ì ìˆ˜: {np.max(all_scores):.3f}\n")
            f.write(f"ìµœì € ì ìˆ˜: {np.min(all_scores):.3f}\n")
            f.write(f"ì„ê³„ê°’({searcher.score_threshold}) ì´ìƒ: {len([s for s in all_scores if s >= searcher.score_threshold])}ê°œ\n")
            f.write(f"95th percentile: {np.percentile(all_scores, 95):.3f}\n")
            f.write(f"90th percentile: {np.percentile(all_scores, 90):.3f}\n")
        else:
            f.write("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\n")
    
    print(f"âœ… ìš”ì•½ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {summary_file}")
    
    # ë²¡í„° DB ì •ë³´ë„ ì €ì¥
    vectordb_info_file = 'result/method2_vectordb_info.txt'
    with open(vectordb_info_file, 'w', encoding='utf-8') as f:
        f.write("=== Method 2: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ===\n")
        f.write(f"ë²¡í„° DB ê²½ë¡œ: {vectordb_dir}\n")
        f.write(f"ì‚¬ìš© ëª¨ë¸: {searcher.model_name}\n")
        f.write(f"ë””ë°”ì´ìŠ¤: {searcher.device}\n")
        f.write(f"ì„ê³„ê°’: {searcher.score_threshold}\n")
        
        # ì¸ë±ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
        import os
        index_file = os.path.join(vectordb_dir, "index.faiss")
        docs_file = os.path.join(vectordb_dir, "docs.jsonl")
        f.write(f"ì¸ë±ìŠ¤ íŒŒì¼ ì¡´ì¬: {os.path.exists(index_file)}\n")
        f.write(f"ë¬¸ì„œ íŒŒì¼ ì¡´ì¬: {os.path.exists(docs_file)}\n")
        
        if os.path.exists(index_file):
            f.write(f"ì¸ë±ìŠ¤ íŒŒì¼ í¬ê¸°: {os.path.getsize(index_file)} bytes\n")
        if os.path.exists(docs_file):
            f.write(f"ë¬¸ì„œ íŒŒì¼ í¬ê¸°: {os.path.getsize(docs_file)} bytes\n")
    
    print(f"âœ… ë²¡í„° DB ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {vectordb_info_file}")
    
    return results_df


if __name__ == "__main__":
    results = main()
