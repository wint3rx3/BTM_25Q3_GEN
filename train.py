from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, PeftModel
from transformers import Trainer as HfTrainer
from datasets import Dataset, Features, Value
import os
import torch
import json

# 0) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ í•œêµ­ì–´ ë§ì¶¤ë²•Â·ë¬¸ì¥ë¶€í˜¸ êµì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
    "- â€˜êµì •í˜•â€™: í‹€ë¦° ë¶€ë¶„ì„ ë°”ë¡œì¡ê³ , ì´ìœ ë¥¼ ì–´ë¬¸ ê·œë²” ê·¼ê±°ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n"
    "- â€˜ì„ íƒí˜•â€™: ë³´ê¸° ì¤‘ ì˜³ì€ í‘œí˜„ì„ ì„ íƒí•˜ê³ , ì´ìœ ë¥¼ ì–´ë¬¸ ê·œë²” ê·¼ê±°ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n"
    "ê²°ê³¼ëŠ” JSONìœ¼ë¡œ, ì•„ë˜ í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”:\n"
    "{\n"
    "  \"id\": \"ë¬¸í•­ID\",\n"
    "  \"output\": {\n"
    "    \"answer\": \"ì •ë‹µ ë¬¸ì¥ ë° ì´ìœ  ì„¤ëª…\"\n"
    "  }\n"
    "}\n"
)

# 1) ëª¨ë¸ ë° LoRA ì„¤ì •
# â”€â”€â”€ Quantization ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,                   # 8-bit ë¡œë”©
    llm_int8_threshold=6.0,              # ì—”ì½”ë”/ë””ì½”ë” ìŠ¤ì¼€ì¼ ê¸°ì¤€ê°’ (ì¡°ì • ê°€ëŠ¥)
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B", trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B", 
    quantization_config=bnb_config,      # quantization_config ì¸ì ì¶”ê°€
    device_map="auto",
    low_cpu_mem_usage=True               # CPU ë©”ëª¨ë¦¬ë„ ì¤„ì—¬ ì¤Œ
)

lora_config = LoraConfig(
    task_type="CAUSAL_LM",
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.05
)
model = get_peft_model(base_model, lora_config)

# âœ… gradient checkpointingì„ ìœ„í•´ í•„ìˆ˜ ì„¤ì •
model.config.use_cache = False

# âœ… base ëª¨ë¸ íŒŒë¼ë¯¸í„° freeze (ë©”ëª¨ë¦¬ ì ˆì•½)
for name, param in model.base_model.named_parameters():
    param.requires_grad = False

# â”€â”€â”€ ì»¤ìŠ¤í…€ Trainer ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Trainer(HfTrainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        # inputs ì—ëŠ” 'input_ids','attention_mask','labels'ê°€ í¬í•¨ë˜ì–´ ìˆìŒ
        labels = inputs.get("labels")
        # PeftModelForCausalLMì€ **inputs ë¡œ labelsë¥¼ ë„˜ê¸°ë©´ lossë¥¼ ë¦¬í„´
        outputs = model(**inputs)
        loss = outputs.loss
        return (loss, outputs) if return_outputs else loss
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 2) ë°ì´í„°ì…‹ ë¡œë“œ
data_files = {
    "train": "data/korean_language_rag_V1.0_train.json",
    "dev":   "data/korean_language_rag_V1.0_dev.json",
    "test":  "data/korean_language_rag_V1.0_test.json"
}

# ë°ì´í„°ì…‹ ìŠ¤í‚¤ë§ˆ ì •ì˜
input_struct = {
    "question":      Value("string"),
    "question_type": Value("string"),
}
features_with_labels = Features({
    "id":    Value("string"),
    "input": input_struct,
    "output": {
        "answer": Value("string")
    }
})
features_no_labels = Features({
    "id":    Value("string"),
    "input": input_struct,
})

# JSON íŒŒì¼ ì§ì ‘ ì½ì–´ Dataset.from_listë¡œ ë³€í™˜
def load_json_dataset(path, features):
    with open(path, encoding="utf-8") as f:
        data = json.load(f)
    return Dataset.from_list(data).cast(features)

train_ds = load_json_dataset(data_files["train"], features_with_labels)
dev_ds   = load_json_dataset(data_files["dev"],   features_with_labels)
test_ds  = load_json_dataset(data_files["test"],  features_no_labels)

# 3) ì „ì²˜ë¦¬
def make_prompt(item):
    return f"[ë¬¸í•­ ID: {item['id']}] ìœ í˜•: {item['input']['question_type']}\n{item['input']['question']}"

def preprocess(ex):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user",   "content": make_prompt(ex)}
    ]
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    target = ex["output"]["answer"]
    full = prompt + target

    tokenized = tokenizer(
        full,
        truncation=True,
        max_length=1024,                 # 2048ì—ì„œ 1024ë¡œ ì¤„ì„ (ë©”ëª¨ë¦¬ ì ˆì•½)
        padding="max_length"
    )

    prompt_len = len(tokenizer(prompt, truncation=True, max_length=1024)["input_ids"])
    labels = [-100] * prompt_len + tokenized["input_ids"][prompt_len:]
    tokenized["labels"] = labels[:1024]  # ensure max_length

    return tokenized

train_ds = train_ds.map(preprocess, remove_columns=train_ds.column_names)
dev_ds   = dev_ds.map(preprocess,   remove_columns=dev_ds.column_names)

# â”€â”€â”€ PyTorch Tensor ë¡œë”©ì„ ìœ„í•œ í¬ë§· ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
train_ds.set_format(type="torch", columns=["input_ids","attention_mask","labels"])
dev_ds.set_format(type="torch",   columns=["input_ids","attention_mask","labels"])
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 4) Trainer ì„¤ì •
training_args = TrainingArguments(
    output_dir="qwen3_correction",
    per_device_train_batch_size=1,         # ğŸ”» ì¤„ì´ê¸° (ê°€ì¥ íš¨ê³¼ í¼)
    gradient_checkpointing=True,           # ğŸ” ë©”ëª¨ë¦¬ ì ˆê°
    bf16=True,                             # ğŸ§  A100ì´ë©´ ì¶”ì²œ (fp16ë³´ë‹¤ ì•ˆì •)
    fp16=False,                            # fp16 ëŒ€ì‹  bf16 ì‚¬ìš©
    # deepspeed="deepspeed_config.json",   # ğŸš€ DeepSpeed ZeRO Stage 2 (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=50,
    report_to="none"                       # wandb ë¹„í™œì„±í™”
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=dev_ds,
    tokenizer=tokenizer,            # (ì„ íƒ) ë¡œê·¸ generation_prompt ë””ì½”ë”©ì— í•„ìš”í•  ìˆ˜ ìˆìŒ
)

trainer.train()
trainer.save_model("qwen3-correction-lora")

# 5) í…ŒìŠ¤íŠ¸ì…‹ ì¶”ë¡ 
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-8B", torch_dtype="auto", device_map="auto")
model = PeftModel.from_pretrained(base_model, "qwen3-correction-lora")
model.eval()

def correct_batch(test_dataset):
    results = []
    for ex in test_dataset:
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user",   "content": make_prompt(ex)}
        ]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        inputs = tokenizer(text, return_tensors="pt", padding=True).to(model.device)
        out = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.6,
            top_p=0.95,
            top_k=20,
            do_sample=False
        )
        raw = tokenizer.decode(out[0], skip_special_tokens=True)
        pred = json.loads(raw)
        results.append({
            "id":     ex["id"],
            "input":  ex["input"],
            "output": pred["output"]
        })
    return results

predictions = correct_batch(test_ds)

# ê²°ê³¼ ì €ì¥ (ì„ íƒ)
with open("predictions.json", "w", encoding="utf-8") as f:
    json.dump(predictions, f, ensure_ascii=False, indent=2)
